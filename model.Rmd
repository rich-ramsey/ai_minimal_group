---
title: "model"
author: "Rich"
date: "2024-05-07"
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

This file builds some models for the AI minimal group experiment.

## load the libraries that we will be using ## 

## install ##

```{r install-pkg}
# install.packages("remotes")
# remotes::install_github("stan-dev/cmdstanr")
# 
# install.packages("devtools")
# devtools::install_github("jmgirard/standist")
# 
# install.packages(c("tidyverse", "RColorBrewer", "patchwork", "brms",
#                     "tidybayes", "bayesplot", "future"))
```

take a snapshot of loaded packages and update the lock.file using renv

```{r snapshot-renv}
# take a snapshot and update the lock.file
# renv::snapshot() # this is only necessary when new packages or installed or packages are updated.
```

## load ##

```{r load-pkg}
pkg <- c("cmdstanr", "standist", "tidyverse", "RColorBrewer", "patchwork", 
         "brms", "tidybayes", "bayesplot", "future", "parallel")

lapply(pkg, library, character.only = TRUE)
```

## settings ##

```{r set-options}
options(brms.backend = "cmdstanr",
        mc.cores = parallel::detectCores(),
        future.fork.enable = TRUE,
        future.rng.onMisuse = "ignore") ## automatically set in RStudio

supportsMulticore()

detectCores()
```

# section 1 - load prior models (if already compiled) and data #

## load prior models, as necessary ##

```{r}
# # index coding model (without separation by group)
# bi1 <- readRDS("models/bi1.rds")

# # index coding models, separated by group (human vs ai)
# bi_h1 <- readRDS("models/bi_h1.rds")
# bi_h2 <- readRDS("models/bi_h2.rds")
# bi_ai1 <- readRDS("models/bi_ai1.rds")
# bi_ai2 <- readRDS("models/bi_ai2.rds")

# # deviation coding models
# bd0.1 <- readRDS("models/bd0.1.rds")
# bd0.2 <- readRDS("models/bd0.2.rds")
# bd1 <- readRDS("models/bd1.rds")
# bd2.1 <- readRDS("models/bd2.1.rds")
# bd2.2 <- readRDS("models/bd2.2.rds")
# bd3.1 <- readRDS("models/bd3.1.rds")
# bd3.2 <- readRDS("models/bd3.2.rds")
# bd3.3 <- readRDS("models/bd3.3.rds")
```

## load data #

```{r}
# deviation coding (numerical so no factors needed). 
# This is good for factorial ANOVA-like designs
diffd <- read_csv("data/diffd.csv")
head(diffd)

# index variable coding
diff <- read_csv("data/diff.csv") %>% 
  mutate(pid = factor(pid, levels = unique(pid)),
         group = factor(group, levels = c("Human", "AI")),
         matrix_name = factor(matrix_name, levels = c("FAVvsMJP", "MDvsMIP/MJP", "FAVvsP"))) %>% 
  select(pid, group, matrix_name, pull)
head(diff)

# now unite into one condition. 
# this will be useful if we want to model each of the 3 levels in condition (per group) to see if it is >0.
# [[this doesn't make sense on reflection, in terms of varying effects, because if we vary condition by pid, then it cannot be possible because of the between group effect of group (AI, Human)]]
diff2 <- diff %>%
  unite("condition", group:matrix_name) %>% 
  mutate(condition = factor(condition, 
                            levels = c("Human_FAVvsMJP", "Human_MDvsMIP/MJP", "Human_FAVvsP",
                                       "AI_FAVvsMJP", "AI_MDvsMIP/MJP", "AI_FAVvsP")))
head(diff2)
str(diff2)

## split by group assignment (Human vs AI), so that we can model them separately and include vary effects of condition/matrix_name by pid
## human
diff_h <- diff %>%
  filter(group == "Human") %>% 
  rename(condition = matrix_name) 
head(diff_h)
str(diff_h)

## ai
diff_ai <- diff %>%
  filter(group == "AI") %>% 
  rename(condition = matrix_name) 
head(diff_ai)
str(diff_ai)
```

## save out the newly constructed diff2 data frame ##

```{r}
# write_csv(diff2, "data/diff2.csv")
# write_csv(diff_h, "data/diff_h.csv")
# write_csv(diff_ai, "data/diff_ai.csv")
```

# section 2 - build models using index coding #

https://solomonkurz.netlify.app/post/2020-12-09-multilevel-models-and-the-index-variable-approach/

## just condition as the index variable ##

this could be useful if we want to estimate conditions as their own intercept. 
e.g., 3 matrices per group, each with an independent intercept rather than an average intercept across all 6?

## bi1 - just condition as a variable ##

## formula ##

```{r}
formula = bf(pull ~ 0 + condition)
```

## check the priors available ##

```{r}
get_prior(formula,
          data = diff2, family = gaussian())
```

## visualise priors ##

here we would normally visualise priors of interest to make a judgment about what would constitute weakly informative priors. 

https://github.com/stan-dev/stan/wiki/Prior-Choice-Recommendations

```{r}
visualize("normal(0, 2)", "normal(0, 1.5)", "normal(0, 1)", "normal(0, 0.5)",
          xlim = c(-4, 4))
```

## set priors ##

```{r}
priors <- c(
  set_prior("normal(0, 1)", class = "b"),
  set_prior("normal(0, 0.5)", class = "sigma") # SD of individual scores
)
```

## run the model ##

```{r}
plan(multicore)
bi1 <- brm(formula = formula,
        data = diff2, family = gaussian(),
        prior = priors,
        iter = 4000, warmup = 2000, cores = 4, chains = 4,
        save_pars = save_pars(all=TRUE),
        seed = 123,
        file = "models/bi1")
summary(bi1)
```

pp_checks

```{r}
ppbi1 <- pp_check(bi1, ndraws = 100)
ppbi1
```

Note - all six effects (3 effects per 2 groups) cannot vary by pid because group is a between participant effect. This is different to the original work that Eliane and Rich did, since there was no between-group manipulation in the original work.

Instead, one way to approach this is to model the data separately per group for this first index coding analysis.
That way we can still include vary effects per pid within each group.
It still addresses our question as to whether all conditions (in each group) are > 0.
And it is the most comparable approach to our original work, which only inlcuded within participant manipulations.

## now split by group and fit the index coding model ##

## human group ##

## bi_h1 - just condition as a variable ##

## formula ##

```{r}
formula = bf(pull ~ 0 + condition)
```

## check the priors available ##

```{r}
get_prior(formula,
          data = diff_h, family = gaussian())
```

## set priors ##

```{r}
priors <- c(
  set_prior("normal(0, 1)", class = "b"),
  set_prior("normal(0, 0.5)", class = "sigma") # SD of individual scores
)
```

## run the model ##

```{r}
plan(multicore)
bi_h1 <- brm(formula = formula,
        data = diff_h, family = gaussian(),
        prior = priors,
        iter = 4000, warmup = 2000, cores = 4, chains = 4,
        save_pars = save_pars(all=TRUE),
        seed = 123,
        file = "models/bi_h1")
summary(bi_h1)
```

pp_checks

```{r}
ppbi_h1 <- pp_check(bi_h1, ndraws = 100)
ppbi_h1
```


## bi_h2 - add varying condition by pid ##

## formula ##

```{r}
formula = bf(pull ~ 0 + condition +
             (0 + condition | pid))
```

## check the priors available ##

```{r}
get_prior(formula,
          data = diff_h, family = gaussian())
```

## set priors ##

```{r}
priors <- c(
  set_prior("normal(0, 1)", class = "b"),
  set_prior("normal(0, 0.5)", class = "sigma"), # SD of individual scores
  set_prior("normal(0, 0.5)", class = "sd"),
  set_prior("lkj(2)", class = "cor") # correlation between varying effects log-units
)
```

## run the model ##

```{r}
plan(multicore)
bi_h2 <- brm(formula = formula,
        data = diff_h, family = gaussian(),
        prior = priors,
        iter = 4000, warmup = 2000, cores = 4, chains = 4,
        control = list(adapt_delta = 0.95), # max_treedepth = 12),
        save_pars = save_pars(all=TRUE),
        seed = 123,
        file = "models/bi_h2")
summary(bi_h2)
```

pp_checks

```{r}
ppbi_h2 <- pp_check(bi_h2, ndraws = 100, type = "dens_overlay_grouped",
                    group = "condition")
ppbi_h2
```

## ai group ##

## bi_ai1 - just condition as a variable ##

## formula ##

```{r}
formula = bf(pull ~ 0 + condition)
```

## check the priors available ##

```{r}
get_prior(formula,
          data = diff_ai, family = gaussian())
```

## set priors ##

```{r}
priors <- c(
  set_prior("normal(0, 1)", class = "b"),
  set_prior("normal(0, 0.5)", class = "sigma") # SD of individual scores
)
```

## run the model ##

```{r}
plan(multicore)
bi_ai1 <- brm(formula = formula,
        data = diff_ai, family = gaussian(),
        prior = priors,
        iter = 4000, warmup = 2000, cores = 4, chains = 4,
        save_pars = save_pars(all=TRUE),
        seed = 123,
        file = "models/bi_ai1")
summary(bi_ai1)
```

pp_checks

```{r}
ppbi_ai1 <- pp_check(bi_ai1, ndraws = 100)
ppbi_ai1
```


## bi_ai2 - add varying condition by pid ##

## formula ##

```{r}
formula = bf(pull ~ 0 + condition +
             (0 + condition | pid))
```

## check the priors available ##

```{r}
get_prior(formula,
          data = diff_ai, family = gaussian())
```

## set priors ##

```{r}
priors <- c(
  set_prior("normal(0, 1)", class = "b"),
  set_prior("normal(0, 0.5)", class = "sigma"), # SD of individual scores
  set_prior("normal(0, 0.5)", class = "sd"),
  set_prior("lkj(2)", class = "cor") # correlation between varying effects log-units
)
```

## run the model ##

```{r}
plan(multicore)
bi_ai2 <- brm(formula = formula,
        data = diff_ai, family = gaussian(),
        prior = priors,
        iter = 4000, warmup = 2000, cores = 4, chains = 4,
        control = list(adapt_delta = 0.95), # max_treedepth = 12),
        save_pars = save_pars(all=TRUE),
        seed = 123,
        file = "models/bi_ai2")
summary(bi_ai2)
```

pp_checks

```{r}
ppbi_ai2 <- pp_check(bi_ai2, ndraws = 100, type = "dens_overlay_grouped",
                    group = "condition")
ppbi_ai2
```

# section 3 - build models using deviation coding #

## bd0.1 intercepts only ##

## formula ##

```{r}
formula = bf(pull ~ 1) 
```

## check the priors available ##

```{r}
get_prior(formula,
          data = diffd, family = gaussian())
```

## visualise priors ##

here we would normally visualise priors of interest to make a judgment about what would constitute weakly informative priors. 

https://github.com/stan-dev/stan/wiki/Prior-Choice-Recommendations

```{r}
visualize("normal(0, 2)", "normal(0, 1.5)", "normal(0, 1)", "normal(0, 0.5)",
          xlim = c(-4, 4))
```

## set priors ##

```{r}
priors <- c(
  set_prior("normal(0, 1)", class = "Intercept"),
  set_prior("normal(0, 0.5)", class = "sigma")
)
```

# run the model #

```{r}
plan(multicore)
bd0.1 <- brm(formula = formula,
        data = diffd, family = gaussian(),
        prior = priors,
        iter = 4000, warmup = 2000, cores = 4, chains = 4,
        save_pars = save_pars(all=TRUE),
        seed = 123,
        file = "models/bd0.1")
summary(bd0.1)
```

## bd0.2 - add varying intercepts by pid ##

## formula ##

```{r}
formula = bf(pull ~ 1 +
            (1 | pid))
```

## check the priors available ##

```{r}
get_prior(formula,
          data = diffd, family = gaussian())
```

## set priors ##

```{r}
priors <- c(
  set_prior("normal(0, 1)", class = "Intercept"),
  set_prior("normal(0, 0.5)", class = "sigma"),
  set_prior("normal(0, 0.5)", class ="sd")
)
```

## run the model ##

```{r}
plan(multicore)
bd0.2 <- brm(formula = formula,
        data = diffd, family = gaussian(),
        prior = priors,
        iter = 4000, warmup = 2000, cores = 4, chains = 4,
        save_pars = save_pars(all=TRUE),
        seed = 123,
        file = "models/bd0.2")
summary(bd0.2)
```

## bd1 - add group ##

the effect of group is a between subject factor and therefore cannot vary by pid.
intercepts can still vary by pid though, of course.

## formula ##

```{r}
formula = bf(pull ~ 1 + group +
               (1 | pid))
```

## check the priors available ##

```{r}
get_prior(formula,
          data = diffd, family = gaussian())
```

## set priors ##

```{r}
priors <- c(
  set_prior("normal(0, 1)", class = "Intercept"),
  set_prior("normal(0, 1)", class = "b"),
  set_prior("normal(0, 0.5)", class = "sigma"),
  set_prior("normal(0, 0.5)", class ="sd")
)
```

## run the model ##

```{r}
plan(multicore)
bd1 <- brm(formula = formula,
        data = diffd, family = gaussian(),
        prior = priors,
        iter = 4000, warmup = 2000, cores = 4, chains = 4,
        save_pars = save_pars(all=TRUE),
        seed = 123,
        file = "models/bd1")
summary(bd1)
```

## bd2.1 - add mat2v1 ##

## formula ##

```{r}
formula = bf(pull ~ 1 + group + mat2v1 +
            (1 + mat2v1 | pid))
```

## check the priors available ##

```{r}
get_prior(formula,
          data = diffd, family = gaussian())
```

## set priors ##

```{r}
priors <- c(
  set_prior("normal(0, 1)", class = "Intercept"),
  set_prior("normal(0, 1)", class = "b"),
  set_prior("normal(0, 0.5)", class = "sigma"),
  set_prior("normal(0, 0.5)", class ="sd"),
  set_prior("lkj(2)", class = "cor") # correlation between varying effects log-units
)
```

## run the model ##

```{r}
plan(multicore)
bd2.1 <- brm(formula = formula,
        data = diffd, family = gaussian(),
        prior = priors,
        iter = 4000, warmup = 2000, cores = 4, chains = 4,
        save_pars = save_pars(all=TRUE),
        seed = 123,
        file = "models/bd2.1")
summary(bd2.1)
```


## bd2.2 - add mat3v1 ##

## formula ##

```{r}
formula = bf(pull ~ 1 + group + mat2v1 + mat3v1 +
            (1 + mat2v1 + mat3v1 | pid)) 
```

## check the priors available ##

```{r}
get_prior(formula,
          data = diffd, family = gaussian())
```

## set priors ##

same as the prior model and no need to re-set.

## run the model ##

```{r}
plan(multicore)
bd2.2 <- brm(formula = formula,
        data = diffd, family = gaussian(),
        prior = priors,
        iter = 4000, warmup = 2000, cores = 4, chains = 4,
        control = list(adapt_delta = 0.95),
        save_pars = save_pars(all=TRUE),
        seed = 123,
        file = "models/bd2.2")
summary(bd2.2)
```


## interaction models ##

## bd3.1 - group*mat2v1 ##

## formula ##

```{r}
formula = bf(pull ~ 1 + group * mat2v1 + mat3v1 +
            (1 + mat2v1 + mat3v1 | pid)) 
```

## run the model ##

```{r}
plan(multicore)
bd3.1 <- brm(formula = formula,
        data = diffd, family = gaussian(),
        prior = priors,
        iter = 4000, warmup = 2000, cores = 4, chains = 4,
        control = list(adapt_delta = 0.95), 
        save_pars = save_pars(all=TRUE),
        seed = 123,
        file = "models/bd3.1")
summary(bd3.1)
```

## bd3.2 - group*mat3v1 ##

## formula ##

```{r}
formula = bf(pull ~ 1 + group * mat3v1 + mat2v1 +
            (1 + mat2v1 + mat3v1 | pid)) 
```

## run the model ##

```{r}
plan(multicore)
bd3.2 <- brm(formula = formula,
        data = diffd, family = gaussian(),
        prior = priors,
        iter = 4000, warmup = 2000, cores = 4, chains = 4,
        control = list(adapt_delta = 0.95), 
        save_pars = save_pars(all=TRUE),
        seed = 123,
        file = "models/bd3.2")
summary(bd3.2)
```

## bd3.3 - group*mat2v1 + group*mat3v1 ##

## formula ##

```{r}
formula = bf(pull ~ 1 + group * mat2v1 + group * mat3v1 +
             (1 + mat2v1 + mat3v1 | pid))
```

## run the model ##

```{r}
plan(multicore)
bd3.3 <- brm(formula = formula,
        data = diffd, family = gaussian(),
        prior = priors,
        iter = 4000, warmup = 2000, cores = 4, chains = 4,
        control = list(adapt_delta = 0.95), 
        save_pars = save_pars(all=TRUE),
        seed = 123,
        file = "models/bd3.3")
summary(bd3.3)
```

pp_checks

```{r}
ppbd3.3 <- pp_check(bd3.3, ndraws = 100)
ppbd3.3

# ppbd3.3b <- pp_check(bd3.3, ndraws = 100, type = "dens_overlay_grouped",
#                     group = "group")
# ppbd3.3b
# 
# ppbd3.3c <- pp_check(bd3.3, ndraws = 100, type = "dens_overlay_grouped",
#                     group = "mat2v1")
# ppbd3.3c
# 
# ppbd3.3d <- pp_check(bd3.3, ndraws = 100, type = "dens_overlay_grouped",
#                     group = "mat3v1")
# ppbd3.3d
```


# model diagnostics #

## look at the chains ##

Here we visualise if the chains mixed reasonably well. 
We do this to see if there are any alarming patterns of non-mixing. 
You should see nicely overlapping caterpillar plots if all is well. 
This is only for the full model as it is the most complex, we would expect the less complex models to behave if this one behaves ok.

## chains for bi1 ##

```{r}
# this adds the chains from model bi2 and creates a posterior samples dataframe called post
post_bi1 <- as_draws_df(bi1)
str(post_bi1)

## here I only focus on the chains for the key variables of interest, but one can easily visualise all the variables if one wishes. Although you'll need a lot of separate figures...

post_bi1 <- post_bi1 %>%
  select(contains(c("b_", ".chain"))) %>% # here I select chains associated with fixed effects and sds. 
  mutate(chain = .chain) 
str(post_bi1)

# now we plot them and save them as necessary
p_chains1 <- post_bi1 %>%
  mcmc_trace(facet_args = list(ncol = 4), window = c(1,2000)) + #  
  scale_x_continuous(breaks = c(0, 2000)) + # if you have 2000 post warm-up samples per chain
  theme_bw() +
  theme(legend.position = "bottom")
p_chains1

# save it
ggsave ("figures/bi1_chains.jpeg",
        width = 6, height = 4)
```

## chains for bi_h2 ##

```{r}
# this adds the chains from model bi2 and creates a posterior samples dataframe called post
post_bi_h2 <- as_draws_df(bi_h2)
str(post_bi_h2)

## here I only focus on the chains for the key variables of interest, but one can easily visualise all the variables if one wishes. Although you'll need a lot of separate figures...

post_bi_h2 <- post_bi_h2 %>%
  select(contains(c("b_", "sd", ".chain"))) %>% # here I select chains associated with fixed effects and sds. 
  mutate(chain = .chain) 
str(post_bi_h2)

# now we plot them and save them as necessary
p_chains2 <- post_bi_h2 %>%
  mcmc_trace(facet_args = list(ncol = 4), window = c(1,2000)) + #  
  scale_x_continuous(breaks = c(0, 2000)) + # if you have 2000 post warm-up samples per chain
  theme_bw() +
  theme(legend.position = "bottom")
p_chains2

# save it
ggsave ("figures/bi_h2_chains.jpeg",
        width = 6, height = 4)
```

## chains for bi_ai2 ##

```{r}
# this adds the chains from model bi2 and creates a posterior samples dataframe called post
post_bi_ai2 <- as_draws_df(bi_ai2)
str(post_bi_ai2)

## here I only focus on the chains for the key variables of interest, but one can easily visualise all the variables if one wishes. Although you'll need a lot of separate figures...

post_bi_ai2 <- post_bi_ai2 %>%
  select(contains(c("b_", "sd", ".chain"))) %>% # here I select chains associated with fixed effects and sds. 
  mutate(chain = .chain) 
str(post_bi_ai2)

# now we plot them and save them as necessary
p_chains3 <- post_bi_ai2 %>%
  mcmc_trace(facet_args = list(ncol = 4), window = c(1,2000)) + #  
  scale_x_continuous(breaks = c(0, 2000)) + # if you have 2000 post warm-up samples per chain
  theme_bw() +
  theme(legend.position = "bottom")
p_chains3

# save it
ggsave ("figures/bi_ai2_chains.jpeg",
        width = 6, height = 4)
```

## chains for bd3.3 ##

```{r}
# this adds the chains from model bd3.3 and creates a posterior samples dataframe called post
post_bd3.3 <- as_draws_df(bd3.3)
str(post_bd3.3)

## here I only focus on the chains for the key variables of interest, but one can easily visualise all the variables if one wishes. Although you'll need a lot of separate figures...

post_bd3.3 <- post_bd3.3 %>%
  select(contains(c("b_", "sd_", ".chain"))) %>% # here I select chains associated with fixed effects and sds
  mutate(chain = .chain) 
str(post_bd3.3)

# now we plot them and save them as necessary
p_chains4 <- post_bd3.3 %>%
  mcmc_trace(facet_args = list(ncol = 4), window = c(1,2000)) + #  
  scale_x_continuous(breaks = c(0, 2000)) + # if you have 2000 post warm-up samples per chain
  theme_bw() +
  theme(legend.position = "bottom")
p_chains4

# save it
ggsave ("figures/bd3.3_chains.jpeg",
        width = 6, height = 4)
```

## other diagnostics ##

## diags for bi1 ##

```{r}
# these two below are worth reporting.
bi1_neff <- mcmc_plot(bi1, type = "neff")
bi1_neff

bi1_rhat <- mcmc_plot(bi1, type = "rhat")
bi1_rhat

# this creates a combined plot
bi1_diag <- bi1_neff / bi1_rhat 
bi1_diag

ggsave("figures/bi1_diag.jpeg",
       width = 6, height = 4)
```

## diags for bi_h2 ##

```{r}
# these two below are worth reporting.
bi_h2_neff <- mcmc_plot(bi_h2, type = "neff")
bi_h2_neff

bi_h2_rhat <- mcmc_plot(bi_h2, type = "rhat")
bi_h2_rhat

# this creates a combined plot
bi_h2_diag <- bi_h2_neff / bi_h2_rhat 
bi_h2_diag

ggsave("figures/bi_h2_diag.jpeg",
       width = 6, height = 4)
```

## diags for bi_ai2 ##

```{r}
# these two below are worth reporting.
bi_ai2_neff <- mcmc_plot(bi_ai2, type = "neff")
bi_ai2_neff

bi_ai2_rhat <- mcmc_plot(bi_ai2, type = "rhat")
bi_ai2_rhat

# this creates a combined plot
bi_ai2_diag <- bi_ai2_neff / bi_ai2_rhat 
bi_ai2_diag

ggsave("figures/bi_ai2_diag.jpeg",
       width = 6, height = 4)
```

## diags for bd3.3 ##

```{r}
# these two below are worth reporting.
bd3.3_neff <- mcmc_plot(bd3.3, type = "neff")
bd3.3_neff

bd3.3_rhat <- mcmc_plot(bd3.3, type = "rhat")
bd3.3_rhat

# this creates a combined plot
bd3.3_diag <- bd3.3_neff / bd3.3_rhat 
bd3.3_diag

ggsave("figures/bd3.3_diag.jpeg",
       width = 6, height = 4)
```

# model comparison #

Here we compute the LOO. Caution: This *may* took some time to compute. 
It depends on the complexity of the model and the amount of data.

## model comparison for bi models ##

```{r}
# set ndraws = 4000, otherwise all samples used and application memory starts to be an issue
plan(multicore)
bi_h1 <- add_criterion(bi_h1, "loo", ndraws = 4000, overwrite = TRUE)
bi_h2 <- add_criterion(bi_h2, "loo", ndraws = 4000, overwrite = TRUE)

bi_ai1 <- add_criterion(bi_ai1, "loo", ndraws = 4000, overwrite = TRUE)
bi_ai2 <- add_criterion(bi_ai2, "loo", ndraws = 4000, overwrite = TRUE)
```

```{r}
print(bi_h1$criteria$loo)
print(bi_h2$criteria$loo)

print(bi_ai1$criteria$loo)
print(bi_aih2$criteria$loo)
```

now compare the models

```{r}
lih <- loo_compare(bi_h1, bi_h2, 
                 criterion = "loo") %>% 
  print(lih, simplify = F)

liai <- loo_compare(bi_h1, bi_h2, 
                 criterion = "loo") %>% 
  print(liai, simplify = F)
```

Here are the loo weights.

```{r}
ih_weights <- model_weights(bi_h1, bi_h2,
              weights = "loo") %>% 
  round(digits = 3)
print(ih_weights, simplify = F)

iai_weights <- model_weights(bi_ai1, bi_ai2,
              weights = "loo") %>% 
  round(digits = 3)
print(iai_weights, simplify = F)
```

plot

human 

```{r}
lih_dat <- lih %>%
  data.frame() %>% 
  rownames_to_column(var = "model")
lih_dat

lih_plot <-  ggplot(lih_dat) +
  geom_pointrange(aes(x = reorder(model, -elpd_loo), y = elpd_loo,
                      ymin = elpd_loo - se_elpd_loo,
                      ymax = elpd_loo + se_elpd_loo,
                      color = model),
                  shape = 16) +
  coord_flip() +
  labs(x = "model", y = "elpd_loo",
       title = "model comparison via Loo") +
  theme_bw() +
  theme(legend.position = "none")
lih_plot
ggsave("figures/loo_ih_plot.jpeg", width = 6, height = 3)
```

ai

```{r}
liai_dat <- liai %>%
  data.frame() %>% 
  rownames_to_column(var = "model")
liai_dat

liai_plot <-  ggplot(liai_dat) +
  geom_pointrange(aes(x = reorder(model, -elpd_loo), y = elpd_loo,
                      ymin = elpd_loo - se_elpd_loo,
                      ymax = elpd_loo + se_elpd_loo,
                      color = model),
                  shape = 16) +
  coord_flip() +
  labs(x = "model", y = "elpd_loo",
       title = "model comparison via Loo") +
  theme_bw() +
  theme(legend.position = "none")
liai_plot
ggsave("figures/loo_iai_plot.jpeg", width = 6, height = 3)
```

## model comparison for bd models ##

```{r}
# set ndraws = 4000, otherwise all samples used and application memory starts to be an issue
plan(multicore)
bd0.1 <- add_criterion(bd0.1, "loo", ndraws = 4000, overwrite = TRUE)
bd0.2 <- add_criterion(bd0.2, "loo", ndraws = 4000, overwrite = TRUE)
bd1 <- add_criterion(bd1, "loo", ndraws = 4000, overwrite = TRUE)
bd2.1 <- add_criterion(bd2.1, "loo", ndraws = 4000, overwrite = TRUE)
bd2.2 <- add_criterion(bd2.2, "loo", ndraws = 4000, overwrite = TRUE)
bd3.1 <- add_criterion(bd3.1, "loo", ndraws = 4000, overwrite = TRUE)
bd3.2 <- add_criterion(bd3.2, "loo", ndraws = 4000, overwrite = TRUE)
bd3.3 <- add_criterion(bd3.3, "loo", ndraws = 4000, overwrite = TRUE)
```

```{r}
print(bd0.1$criteria$loo)
print(bd0.2$criteria$loo)
print(bd1$criteria$loo)
print(bd2.1$criteria$loo)
print(bd2.2$criteria$loo)
print(bd3.1$criteria$loo)
print(bd3.2$criteria$loo)
print(bd3.3$criteria$loo)
```

now compare the models

```{r}
ld <- loo_compare(bd0.1, bd0.2, bd1, bd2.1, bd2.2, bd3.1, bd3.2, bd3.3,  
                 criterion = "loo") %>% 
  print(ld, simplify = F)
```

Here are the loo weights.

```{r}
d_weights <- model_weights(bd0.1, bd0.2, bd1, bd2.1, bd2.2, bd3.1, bd3.2, bd3.3,
              weights = "loo") %>% 
  round(digits = 3)
print(d_weights, simplify = F)
```

plot

```{r}
ld_dat <- ld %>%
  data.frame() %>% 
  rownames_to_column(var = "model")
ld_dat

ld_plot <-  ggplot(ld_dat) +
  geom_pointrange(aes(x = reorder(model, -elpd_loo), y = elpd_loo,
                      ymin = elpd_loo - se_elpd_loo,
                      ymax = elpd_loo + se_elpd_loo,
                      color = model),
                  shape = 16) +
  coord_flip() +
  labs(x = "model", y = "elpd_loo",
       title = "model comparison via Loo") +
  theme_bw() +
  theme(legend.position = "none")
ld_plot
ggsave("figures/loo_d_plot.jpeg", width = 6, height = 3)
```

